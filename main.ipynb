{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a06afa",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition System  \n",
    "### Computer Vision Project using MediaPipe Hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: IMPORTS AND INITIALIZATION\n",
    "import cv2                     \n",
    "import mediapipe as mp          # MediaPipe for hand detection \n",
    "import numpy as np              \n",
    "import time                     \n",
    "import pandas as pd             \n",
    "from collections import defaultdict  \n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# Initialize MediaPipe Hands solution\n",
    "# This provides pre-trained models for detecting hand landmarks in real-time\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774b2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: HELPER FUNCTIONS FOR FINGER DETECTION\n",
    "\n",
    "def is_finger_extended(landmarks, finger_tip_id, finger_pip_id):\n",
    "    \"\"\"\n",
    "    Check if a specific finger is extended based on landmark positions\n",
    "    Returns:\n",
    "        Boolean: True if finger is extended, False if curled\n",
    "    Logic:\n",
    "        - If the tip is above (smaller y-value) the PIP joint, finger is extended\n",
    "        - MediaPipe uses normalized coordinates where y=0 is top, y=1 is bottom\n",
    "    \"\"\"\n",
    "    tip = landmarks[finger_tip_id]\n",
    "    pip = landmarks[finger_pip_id]\n",
    "    return tip.y < pip.y \n",
    "\n",
    "def is_thumb_extended(landmarks):\n",
    "    \"\"\"\n",
    "    Special case for thumb - check horizontal extension relative to palm\n",
    "    Returns:Boolean: True if thumb is extended, False if curled\n",
    "    \"\"\"\n",
    "    thumb_tip = landmarks[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    thumb_ip = landmarks[mp_hands.HandLandmark.THUMB_IP]\n",
    "    thumb_mcp = landmarks[mp_hands.HandLandmark.THUMB_MCP]\n",
    "    \n",
    "    # Get the palm center x-coordinate (wrist position)\n",
    "    palm_center_x = landmarks[mp_hands.HandLandmark.WRIST].x\n",
    "    \n",
    "    # Thumb is extended if its tip is further from wrist than its base\n",
    "    return abs(thumb_tip.x - palm_center_x) > abs(thumb_mcp.x - palm_center_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e85634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: GESTURE CLASSIFICATION LOGIC\n",
    "\n",
    "def classify_gesture(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Classify a detected hand into one of the predefined gestures\n",
    "    \n",
    "    Uses a rule-based system:\n",
    "    1. Extract all finger extension states using helper functions\n",
    "    2. Apply rules to match specific gesture patterns\n",
    "    3. Return the recognized gesture or \"UNKNOWN\"\n",
    "    \n",
    "    Recognized gestures:\n",
    "        - FIST: No fingers extended\n",
    "        - OPEN_PALM: All fingers extended\n",
    "        - THUMBS_UP: Only thumb extended\n",
    "        - PEACE_SIGN: Index and middle finger extended\n",
    "        - POINTING: Only index finger extended\n",
    "        - UNKNOWN: Doesn't match any pattern\n",
    "    Returns:\n",
    "        String: Name of the recognized gesture\n",
    "    \"\"\"\n",
    "    landmarks = hand_landmarks.landmark\n",
    "\n",
    "    # Determine which fingers are extended using the helper functions\n",
    "    thumb_extended = is_thumb_extended(landmarks)\n",
    "    index_extended = is_finger_extended(landmarks,\n",
    "                                        mp_hands.HandLandmark.INDEX_FINGER_TIP,\n",
    "                                        mp_hands.HandLandmark.INDEX_FINGER_PIP)\n",
    "    middle_extended = is_finger_extended(landmarks,\n",
    "                                         mp_hands.HandLandmark.MIDDLE_FINGER_TIP,\n",
    "                                         mp_hands.HandLandmark.MIDDLE_FINGER_PIP)\n",
    "    ring_extended = is_finger_extended(landmarks,\n",
    "                                       mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "                                       mp_hands.HandLandmark.RING_FINGER_PIP)\n",
    "    pinky_extended = is_finger_extended(landmarks,\n",
    "                                        mp_hands.HandLandmark.PINKY_TIP,\n",
    "                                        mp_hands.HandLandmark.PINKY_PIP)\n",
    "\n",
    "    # Apply gesture classification rules\n",
    "    # FIST: No fingers extended\n",
    "    if not any([thumb_extended, index_extended, middle_extended, ring_extended, pinky_extended]):\n",
    "        return \"FIST\"\n",
    "    \n",
    "    # OPEN_PALM: All fingers extended\n",
    "    if all([thumb_extended, index_extended, middle_extended, ring_extended, pinky_extended]):\n",
    "        return \"OPEN_PALM\"\n",
    "    \n",
    "    # THUMBS_UP: Only thumb extended\n",
    "    if thumb_extended and not any([index_extended, middle_extended, ring_extended, pinky_extended]):\n",
    "        return \"THUMBS_UP\"\n",
    "    \n",
    "    # PEACE_SIGN: Index and middle extended, others curled\n",
    "    if index_extended and middle_extended and not any([ring_extended, pinky_extended]):\n",
    "        return \"PEACE_SIGN\"\n",
    "    \n",
    "    # POINTING: Only index finger extended\n",
    "    if index_extended and not any([middle_extended, ring_extended, pinky_extended]):\n",
    "        return \"POINTING\"\n",
    "\n",
    "    # If no pattern matches, return UNKNOWN\n",
    "    return \"UNKNOWN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5897b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 4: REAL-TIME GESTURE RECOGNITION FUNCTION\n",
    "\n",
    "def run_gesture_recognition(duration=60, show_landmarks=True):\n",
    "    \"\"\"\n",
    "    Capture video from webcam and perform real-time gesture recognition\n",
    "    \n",
    "    This function:\n",
    "    1. Opens the default webcam (camera index 0)\n",
    "    2. Processes each video frame to detect hand landmarks\n",
    "    3. Classifies detected hand gestures in real-time\n",
    "    4. Displays the video with landmark overlays and gesture predictions\n",
    "    5. Runs for a specified duration or until user presses 'q'\n",
    "\n",
    "    Args:\n",
    "        duration: How many seconds to run (default 60 seconds)\n",
    "        show_landmarks: If True, draw hand skeleton on video (default True)\n",
    "    \n",
    "    Controls:\n",
    "        Press 'q' to quit early\n",
    "    \"\"\"\n",
    "    # Initialize video capture from default webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    start_time = time.time()\n",
    "    current_gesture = \"NONE\"  # Track the currently detected gesture\n",
    "\n",
    "    print(\"Starting gesture recognition...\")\n",
    "    print(\"Press 'q' to quit\\n\")\n",
    "\n",
    "    # Main recognition loop\n",
    "    while cap.isOpened() and (time.time() - start_time) < duration:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip frame horizontally for selfie view (mirror effect)\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Convert BGR (OpenCV default) to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process frame with MediaPipe hands detector\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        # If hands are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand skeleton (21 landmarks connected by lines)\n",
    "                if show_landmarks:\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Classify the gesture\n",
    "                gesture = classify_gesture(hand_landmarks)\n",
    "                \n",
    "                # Print only when gesture changes (to avoid spam)\n",
    "                if gesture != current_gesture:\n",
    "                    current_gesture = gesture\n",
    "                    print(\"Detected:\", current_gesture)\n",
    "        else:\n",
    "            # No hand detected\n",
    "            if current_gesture != \"NONE\":\n",
    "                current_gesture = \"NONE\"\n",
    "                print(\"No hand detected\")\n",
    "\n",
    "        # Display current gesture on the video frame\n",
    "        cv2.putText(frame, f\"Gesture: {current_gesture}\",\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1, (0, 255, 0), 2)\n",
    "\n",
    "        # Show the video window\n",
    "        cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "\n",
    "        # Check for 'q' key press to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Cleanup: release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6c0280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gesture recognition...\n",
      "Press 'q' to quit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 5: RUN BASIC GESTURE RECOGNITION\n",
    "run_gesture_recognition(duration=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f3df686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for systematically collecting and evaluating gesture recognition performance\n",
    "class GestureEvaluator:\n",
    "    \"\"\"\n",
    "    Class for collecting gesture evaluation data and calculating performance metrics\n",
    "    \n",
    "    Purpose:\n",
    "    - Collect labeled samples of different gestures under various conditions\n",
    "    - Track ground truth vs predicted labels\n",
    "    - Calculate accuracy metrics at multiple levels (overall, by gesture, by condition)\n",
    "    - Generate confusion matrices to understand misclassifications\n",
    "    - Save/load evaluation results to/from CSV files\n",
    "    \n",
    "    Data collected:\n",
    "        For each sample: timestamp, ground truth gesture, predicted gesture,\n",
    "        correctness, distance (close/medium/far), background (clean/cluttered),\n",
    "        and model confidence score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the evaluator with empty data list and gesture names\"\"\"\n",
    "        self.evaluation_data = []  # List to store all evaluation samples\n",
    "        self.gesture_names = [\"FIST\", \"OPEN_PALM\", \"THUMBS_UP\", \"PEACE_SIGN\", \"POINTING\", \"UNKNOWN\"]\n",
    "\n",
    "    def collect_evaluation_data(self, gesture_name, num_samples=30,\n",
    "                                distance=\"medium\", background=\"clean\"):\n",
    "        \n",
    "        \"\"\"Collect labeled samples of a specific gesture under specific conditions\"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        samples_collected = 0\n",
    "        collecting = False\n",
    "\n",
    "        print(f\"\\nCollecting data for: {gesture_name}\")\n",
    "        print(f\"Distance: {distance}, Background: {background}\")\n",
    "        print(\"Press 's' to start, 'q' to quit\\n\")\n",
    "\n",
    "        while cap.isOpened() and samples_collected < num_samples:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(rgb_frame)\n",
    "\n",
    "            predicted = \"NONE\"\n",
    "            confidence_score = 0.0\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    # Draw hand skeleton on frame\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    \n",
    "                    # Classify the gesture\n",
    "                    predicted = classify_gesture(hand_landmarks)\n",
    "                    \n",
    "                    # Get confidence score from MediaPipe's hand detection\n",
    "                    confidence_score = results.multi_handedness[0].classification[0].score\n",
    "\n",
    "                    # If actively collecting, save this sample\n",
    "                    if collecting:\n",
    "                        self.evaluation_data.append({\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                            'ground_truth': gesture_name,\n",
    "                            'predicted': predicted,\n",
    "                            'correct': predicted == gesture_name,\n",
    "                            'distance': distance,\n",
    "                            'background': background,\n",
    "                            'confidence': confidence_score\n",
    "                        })\n",
    "                        samples_collected += 1\n",
    "\n",
    "            # Display status on frame\n",
    "            status = \"COLLECTING\" if collecting else \"READY - Press 's'\"\n",
    "            cv2.putText(frame, f\"{status} ({samples_collected}/{num_samples})\",\n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, f\"Ground Truth: {gesture_name}\",\n",
    "                        (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, f\"Predicted: {predicted}\",\n",
    "                        (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "            cv2.imshow(\"Data Collection\", frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            if key == ord('s'):\n",
    "                collecting = True\n",
    "                print(\"Started collecting...\")\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f\"Collected {samples_collected} samples for {gesture_name}\\n\")\n",
    "\n",
    "    def run_full_evaluation(self, samples_per_condition=30):\n",
    "        \"\"\"\n",
    "        Run a comprehensive evaluation across all \n",
    "        gesture/distance/background combinations\n",
    "        \n",
    "        Tests every gesture with every distance and background combination\n",
    "        This creates a complete picture of model robustness\n",
    "        \"\"\"\n",
    "        distances = [\"close\", \"medium\", \"far\"]\n",
    "        backgrounds = [\"clean\", \"cluttered\"]\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        print(\" FULL GESTURE EVALUATION \")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        for gesture in self.gesture_names:\n",
    "            for distance in distances:\n",
    "                for background in backgrounds:\n",
    "                    print(f\"Testing {gesture} | {distance} | {background}\")\n",
    "                    self.collect_evaluation_data(\n",
    "                        gesture,\n",
    "                        num_samples=samples_per_condition,\n",
    "                        distance=distance,\n",
    "                        background=background\n",
    "                    )\n",
    "                    time.sleep(1)\n",
    "\n",
    "        # Save results after all data collection\n",
    "        self.save_results()\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculate and display gesture recognition accuracy metrics\n",
    "        Prints detailed results and returns a dictionary for further analysis\n",
    "        \"\"\"\n",
    "        if not self.evaluation_data:\n",
    "            print(\"No evaluation data found.\")\n",
    "            return\n",
    "\n",
    "        # Convert evaluation data list to pandas DataFrame for easy analysis\n",
    "        df = pd.DataFrame(self.evaluation_data)\n",
    "\n",
    "        # Calculate accuracy at different levels\n",
    "        overall_accuracy = df[\"correct\"].mean() * 100  # Percentage of all correct predictions\n",
    "        gesture_accuracy = df.groupby(\"ground_truth\")[\"correct\"].mean() * 100  # Per gesture\n",
    "        distance_accuracy = df.groupby(\"distance\")[\"correct\"].mean() * 100  # Per distance\n",
    "        background_accuracy = df.groupby(\"background\")[\"correct\"].mean() * 100  # Per background\n",
    "\n",
    "        # Create confusion matrix: shows which gestures are confused with which\n",
    "        # Each row is a ground truth gesture, each column is a predicted gesture\n",
    "        confusion = pd.crosstab(df[\"ground_truth\"], df[\"predicted\"],\n",
    "                                normalize=\"index\") * 100\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\nOverall Accuracy:\", round(overall_accuracy, 2), \"%\")\n",
    "        print(\"\\nGesture Accuracy:\\n\", gesture_accuracy)\n",
    "        print(\"\\nDistance Accuracy:\\n\", distance_accuracy)\n",
    "        print(\"\\nBackground Accuracy:\\n\", background_accuracy)\n",
    "        print(\"\\nConfusion Matrix:\\n\", confusion.round(2))\n",
    "\n",
    "        return {\n",
    "            \"overall_accuracy\": overall_accuracy,\n",
    "            \"gesture_accuracy\": gesture_accuracy.to_dict(),\n",
    "            \"distance_accuracy\": distance_accuracy.to_dict(),\n",
    "            \"background_accuracy\": background_accuracy.to_dict(),\n",
    "            \"confusion\": confusion.to_dict()\n",
    "        }\n",
    "\n",
    "    def save_results(self, filename=\"evaluation_results.csv\"):\n",
    "        \"\"\"\n",
    "     Save evaluation data to CSV file for later analysis\n",
    "        Output:\n",
    "            CSV file with columns:\n",
    "            timestamp, ground_truth, predicted, correct, distance, background, confidence\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self.evaluation_data)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "\n",
    "    def load_results(self, filename=\"evaluation_results.csv\"):\n",
    "        \"\"\"\n",
    "        Load previously saved evaluation results from CSV file\n",
    "        filename: Name of CSV file to load (default \"evaluation_results.csv\")\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(filename)\n",
    "        self.evaluation_data = df.to_dict(\"records\")\n",
    "        print(f\"Loaded {len(self.evaluation_data)} samples from {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "980e1e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running quick robustness test...\n",
      "This will test each gesture under different conditions\n",
      "Follow on-screen prompts to collect samples\n",
      "\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "\n",
      "==================================================\n",
      "No evaluation data found.\n",
      "Results saved to robustness_test.csv\n",
      "==================================================\n",
      "✓ Robustness test complete!\n",
      "✓ Results saved to: robustness_test.csv\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "\n",
      "==================================================\n",
      "No evaluation data found.\n",
      "Results saved to robustness_test.csv\n",
      "==================================================\n",
      "✓ Robustness test complete!\n",
      "✓ Results saved to: robustness_test.csv\n"
     ]
    }
   ],
   "source": [
    "# PART 7: TESTING AND CSV GENERATION - CHOOSE ONE OPTION\n",
    "# ============================================================================\n",
    "# OPTION 1: Quick Robustness Test (Recommended for first-time testing)\n",
    "# ============================================================================\n",
    "# Run a quick test on a subset of gestures, collect 10 samples each\n",
    "# Creates: robustness_test.csv\n",
    "#\n",
    "# To use: Uncomment and run this cell\n",
    "\n",
    "def test_robustness():\n",
    "    \"\"\"\n",
    "    Run a quick robustness test and save results to CSV\n",
    "    \n",
    "    Steps:\n",
    "    1. Creates a GestureEvaluator instance\n",
    "    2. Tests 6 gestures x 3 conditions (close/far/clean/cluttered) = 6 conditions total\n",
    "    3. Collects 10 samples per gesture+condition combination\n",
    "    4. Calculates accuracy metrics and displays them\n",
    "    5. Saves all data to robustness_test.csv\n",
    "    \"\"\"\n",
    "    evaluator = GestureEvaluator()\n",
    "\n",
    "    print(\"Running quick robustness test...\")\n",
    "    print(\"This will test each gesture under different conditions\")\n",
    "    print(\"Follow on-screen prompts to collect samples\\n\")\n",
    "\n",
    "    # Define test cases: (gesture, distance, background)\n",
    "    test_cases = [\n",
    "        (\"FIST\", \"close\", \"clean\"),\n",
    "        (\"FIST\", \"far\", \"cluttered\"),\n",
    "        (\"OPEN_PALM\", \"medium\", \"clean\"),\n",
    "        (\"THUMBS_UP\", \"close\", \"cluttered\"),\n",
    "        (\"PEACE_SIGN\", \"far\", \"clean\"),\n",
    "        (\"POINTING\", \"medium\", \"cluttered\"),\n",
    "    ]\n",
    "\n",
    "    # Collect samples for each test case\n",
    "    for gesture, dist, bg in test_cases:\n",
    "        evaluator.collect_evaluation_data(\n",
    "            gesture, num_samples=10, distance=dist, background=bg\n",
    "        )\n",
    "\n",
    "    # Calculate and display metrics\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    metrics = evaluator.calculate_metrics()\n",
    "    \n",
    "    # CRITICAL: Save results to CSV file\n",
    "    evaluator.save_results(\"robustness_test.csv\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"✓ Robustness test complete!\")\n",
    "    print(\"✓ Results saved to: robustness_test.csv\")\n",
    "\n",
    "# This will run the robustness test when the cell is executed\n",
    "test_robustness()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY: How to Generate CSV Files\n",
    "# ============================================================================\n",
    "# 1. Choose ONE option above and uncomment it\n",
    "# 2. Run the cell (Shift+Enter or click Run)\n",
    "# 3. Follow on-screen prompts:\n",
    "#    - Press 's' to START collecting samples\n",
    "#    - Press 'q' to QUIT\n",
    "# 4. Wait for the function to finish\n",
    "# 5. A CSV file will be created in your project folder:\n",
    "#    - robustness_test.csv (from Option 1)\n",
    "#    - evaluation_results.csv (from Option 2)\n",
    "#    - my_evaluation.csv (from Option 3)\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118b0c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " FULL GESTURE EVALUATION \n",
      "==================================================\n",
      "Testing FIST | close | clean\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Testing FIST | close | cluttered\n",
      "Testing FIST | close | cluttered\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Testing FIST | medium | clean\n",
      "Testing FIST | medium | clean\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Testing FIST | medium | cluttered\n",
      "Testing FIST | medium | cluttered\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Testing FIST | far | clean\n",
      "Testing FIST | far | clean\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Testing FIST | far | cluttered\n",
      "Testing FIST | far | cluttered\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: FIST\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Testing OPEN_PALM | close | clean\n",
      "Testing OPEN_PALM | close | clean\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Testing OPEN_PALM | close | cluttered\n",
      "Testing OPEN_PALM | close | cluttered\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Testing OPEN_PALM | medium | clean\n",
      "Testing OPEN_PALM | medium | clean\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Testing OPEN_PALM | medium | cluttered\n",
      "Testing OPEN_PALM | medium | cluttered\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Testing OPEN_PALM | far | clean\n",
      "Testing OPEN_PALM | far | clean\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Testing OPEN_PALM | far | cluttered\n",
      "Testing OPEN_PALM | far | cluttered\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Testing THUMBS_UP | close | clean\n",
      "Testing THUMBS_UP | close | clean\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Testing THUMBS_UP | close | cluttered\n",
      "Testing THUMBS_UP | close | cluttered\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Testing THUMBS_UP | medium | clean\n",
      "Testing THUMBS_UP | medium | clean\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Testing THUMBS_UP | medium | cluttered\n",
      "Testing THUMBS_UP | medium | cluttered\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Testing THUMBS_UP | far | clean\n",
      "Testing THUMBS_UP | far | clean\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Testing THUMBS_UP | far | cluttered\n",
      "Testing THUMBS_UP | far | cluttered\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "Testing PEACE_SIGN | close | clean\n",
      "Testing PEACE_SIGN | close | clean\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Testing PEACE_SIGN | close | cluttered\n",
      "Testing PEACE_SIGN | close | cluttered\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Testing PEACE_SIGN | medium | clean\n",
      "Testing PEACE_SIGN | medium | clean\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Testing PEACE_SIGN | medium | cluttered\n",
      "Testing PEACE_SIGN | medium | cluttered\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Testing PEACE_SIGN | far | clean\n",
      "Testing PEACE_SIGN | far | clean\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Testing PEACE_SIGN | far | cluttered\n",
      "Testing PEACE_SIGN | far | cluttered\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: PEACE_SIGN\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Collected 0 samples for PEACE_SIGN\n",
      "\n",
      "Testing POINTING | close | clean\n",
      "Testing POINTING | close | clean\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "Testing POINTING | close | cluttered\n",
      "Testing POINTING | close | cluttered\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "Testing POINTING | medium | clean\n",
      "Testing POINTING | medium | clean\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alija\\Desktop\\School-Folder\\4337_Intro_Computer_Vision\\Hand-Gesture-Output\\.venv\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 0 samples for POINTING\n",
      "\n",
      "Testing POINTING | medium | cluttered\n",
      "Testing POINTING | medium | cluttered\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "Testing POINTING | far | clean\n",
      "Testing POINTING | far | clean\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "Testing POINTING | far | cluttered\n",
      "Testing POINTING | far | cluttered\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: POINTING\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "Collected 0 samples for POINTING\n",
      "\n",
      "Testing UNKNOWN | close | clean\n",
      "Testing UNKNOWN | close | clean\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Testing UNKNOWN | close | cluttered\n",
      "Testing UNKNOWN | close | cluttered\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: close, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Testing UNKNOWN | medium | clean\n",
      "Testing UNKNOWN | medium | clean\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: medium, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Testing UNKNOWN | medium | cluttered\n",
      "Testing UNKNOWN | medium | cluttered\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Testing UNKNOWN | far | clean\n",
      "Testing UNKNOWN | far | clean\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Testing UNKNOWN | far | cluttered\n",
      "Testing UNKNOWN | far | cluttered\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: UNKNOWN\n",
      "Distance: far, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Collected 0 samples for UNKNOWN\n",
      "\n",
      "Results saved to evaluation_results.csv\n",
      "Results saved to evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# OPTION 2: Full Evaluation (Complete and thorough)\n",
    "# Test ALL gestures with ALL distance/background combinations\n",
    "# Creates: evaluation_results.csv\n",
    "\n",
    "evaluator = GestureEvaluator()\n",
    "evaluator.run_full_evaluation(samples_per_condition=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644b7a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting data for: FIST\n",
      "Distance: close, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "Collected 0 samples for FIST\n",
      "\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: OPEN_PALM\n",
      "Distance: medium, Background: cluttered\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "Collected 0 samples for OPEN_PALM\n",
      "\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "\n",
      "Collecting data for: THUMBS_UP\n",
      "Distance: far, Background: clean\n",
      "Press 's' to start, 'q' to quit\n",
      "\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "No evaluation data found.\n",
      "Results saved to my_evaluation.csv\n",
      "Collected 0 samples for THUMBS_UP\n",
      "\n",
      "No evaluation data found.\n",
      "Results saved to my_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "# OPTION 3: Custom Evaluation (Most flexible)\n",
    "# Manually collect data for specific gestures and conditions\n",
    "\n",
    "evaluator = GestureEvaluator()\n",
    "\n",
    "# # Collect data for specific gestures\n",
    "evaluator.collect_evaluation_data(\"FIST\", num_samples=20, distance=\"close\", background=\"clean\")\n",
    "evaluator.collect_evaluation_data(\"OPEN_PALM\", num_samples=20, distance=\"medium\", background=\"cluttered\")\n",
    "evaluator.collect_evaluation_data(\"THUMBS_UP\", num_samples=20, distance=\"far\", background=\"clean\")\n",
    "# \n",
    "#Calculate metrics\n",
    "evaluator.calculate_metrics()\n",
    " \n",
    "#Save to CSV (IMPORTANT!)\n",
    "evaluator.save_results(\"my_evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bdc06",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# OPTION 4: Load and Analyze Previous Results\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Load results from an existing CSV file and analyze them\u001b[39;00m\n\u001b[0;32m      4\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m GestureEvaluator()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluation_results.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load existing results\u001b[39;00m\n\u001b[0;32m      6\u001b[0m evaluator\u001b[38;5;241m.\u001b[39mcalculate_metrics()  \u001b[38;5;66;03m# Display analysis\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 180\u001b[0m, in \u001b[0;36mGestureEvaluator.load_results\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    Load previously saved evaluation results from CSV file\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    filename: Name of CSV file to load (default \"evaluation_results.csv\")\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alija\\Desktop\\School-Folder\\4337_Intro_Computer_Vision\\Hand-Gesture-Output\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alija\\Desktop\\School-Folder\\4337_Intro_Computer_Vision\\Hand-Gesture-Output\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\alija\\Desktop\\School-Folder\\4337_Intro_Computer_Vision\\Hand-Gesture-Output\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alija\\Desktop\\School-Folder\\4337_Intro_Computer_Vision\\Hand-Gesture-Output\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alija\\Desktop\\School-Folder\\4337_Intro_Computer_Vision\\Hand-Gesture-Output\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# OPTION 4: Load and Analyze Previous Results\n",
    "# Load results from an existing CSV file and analyze them\n",
    "\n",
    "evaluator = GestureEvaluator()\n",
    "evaluator.load_results(\"Our-test/evaluation_results.csv\")  # Load existing results\n",
    "evaluator.calculate_metrics()  # Display analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 8: PLOTTING UTILITIES — Generate Figures & Tables from Evaluation Data\n",
    "# Add graphs/tables for evaluation results (accuracy, confusion matrix, counts)\n",
    "# Requires: matplotlib (already in requirements.txt), seaborn (optional, improves heatmap)\n",
    "\n",
    "def plot_evaluation_results(evaluator, save_dir=\"results_plots\", show_figs=False):\n",
    "    \"\"\"\n",
    "    Create and save plots summarizing evaluator.evaluation_data.\n",
    "    Produces PNG files:\n",
    "        - per_gesture_accuracy.png\n",
    "        - accuracy_by_distance.png (if 'distance' present)\n",
    "        - accuracy_by_background.png (if 'background' present)\n",
    "        - confusion_matrix.png\n",
    "        - predictions_count.png\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Ensure data is present\n",
    "    if not hasattr(evaluator, 'evaluation_data'):\n",
    "        raise ValueError('Evaluator object has no attribute evaluation_data')\n",
    "\n",
    "    df = pd.DataFrame(evaluator.evaluation_data)\n",
    "    if df.empty:\n",
    "        print('No evaluation data to plot. Collect data or load a CSV first.')\n",
    "        return\n",
    "\n",
    "    # Normalize expected boolean column\n",
    "    if 'correct' in df.columns:\n",
    "        df['correct'] = df['correct'].astype(bool)\n",
    "\n",
    "    # 1) Per-gesture accuracy\n",
    "    per_gesture = (df.groupby('ground_truth')['correct'].mean() * 100).sort_values(ascending=False)\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    per_gesture.plot(kind='bar', color='tab:blue', ax=ax)\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('Per-Gesture Accuracy')\n",
    "    ax.set_ylim(0, 100)\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{p.get_height():.1f}%\", (p.get_x() + p.get_width() / 2, p.get_height()),\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    fn = os.path.join(save_dir, 'per_gesture_accuracy.png')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fn, dpi=150)\n",
    "    if show_figs: plt.show()\n",
    "    plt.close(fig)\n",
    "    print('Saved:', fn)\n",
    "\n",
    "    # 2) Accuracy by distance\n",
    "    if 'distance' in df.columns:\n",
    "        order = ['close', 'medium', 'far']\n",
    "        distance_acc = (df.groupby('distance')['correct'].mean() * 100).reindex(order)\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        distance_acc.plot(kind='bar', color='tab:green', ax=ax)\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title('Accuracy by Distance')\n",
    "        ax.set_ylim(0, 100)\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f\"{(p.get_height() if not np.isnan(p.get_height()) else 0):.1f}%\",\n",
    "                        (p.get_x() + p.get_width() / 2, (p.get_height() if not np.isnan(p.get_height()) else 0)),\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "        fn = os.path.join(save_dir, 'accuracy_by_distance.png')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(fn, dpi=150)\n",
    "        if show_figs: plt.show()\n",
    "        plt.close(fig)\n",
    "        print('Saved:', fn)\n",
    "\n",
    "    # 3) Accuracy by background \n",
    "    if 'background' in df.columns:\n",
    "        bg_acc = (df.groupby('background')['correct'].mean() * 100)\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        bg_acc.plot(kind='bar', color='tab:orange', ax=ax)\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title('Accuracy by Background')\n",
    "        ax.set_ylim(0, 100)\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f\"{p.get_height():.1f}%\", (p.get_x() + p.get_width() / 2, p.get_height()),\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "        fn = os.path.join(save_dir, 'accuracy_by_background.png')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(fn, dpi=150)\n",
    "        if show_figs: plt.show()\n",
    "        plt.close(fig)\n",
    "        print('Saved:', fn)\n",
    "\n",
    "    # 4) Confusion matrix (percentage per ground-truth row)\n",
    "    cm = pd.crosstab(df['ground_truth'], df['predicted'], normalize='index') * 100\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    im = ax.imshow(cm.fillna(0).values, cmap='viridis', vmin=0, vmax=100)\n",
    "    ax.set_xticks(np.arange(len(cm.columns)))\n",
    "    ax.set_yticks(np.arange(len(cm.index)))\n",
    "    ax.set_xticklabels(cm.columns, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(cm.index)\n",
    "    for i in range(len(cm.index)):\n",
    "        for j in range(len(cm.columns)):\n",
    "            val = cm.fillna(0).iloc[i, j]\n",
    "            ax.text(j, i, f\"{val:.1f}%\", ha='center', va='center', color='white', fontsize=8)\n",
    "    fig.colorbar(im, ax=ax, label='Percent')\n",
    "    ax.set_title('Confusion Matrix (rows = ground truth, cols = predicted)')\n",
    "    fig.tight_layout()\n",
    "    fn = os.path.join(save_dir, 'confusion_matrix.png')\n",
    "    fig.savefig(fn, dpi=150)\n",
    "    if show_figs: plt.show()\n",
    "    plt.close(fig)\n",
    "    print('Saved:', fn)\n",
    "\n",
    "    # 5) Predicted label counts\n",
    "    counts = df['predicted'].value_counts()\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    counts.plot(kind='bar', color='tab:purple', ax=ax)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Predicted Label Counts')\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{int(p.get_height())}\", (p.get_x() + p.get_width() / 2, p.get_height()),\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    fn = os.path.join(save_dir, 'predictions_count.png')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fn, dpi=150)\n",
    "    if show_figs: plt.show()\n",
    "    plt.close(fig)\n",
    "    print('Saved:', fn)\n",
    "\n",
    "    print('\\nAll plots saved in:', save_dir)\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Small helper: load CSV and plot in one call\n",
    "# -------------------\n",
    "\n",
    "def load_results_and_plot(csv_filename, save_dir='results_plots', show_figs=False):\n",
    "    #Load CSV into a GestureEvaluator-like object and call plot_evaluation_results.\n",
    "\n",
    "    class _TmpEvaluator:\n",
    "        pass\n",
    "\n",
    "    tmp = _TmpEvaluator()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename)\n",
    "    except Exception as e:\n",
    "        print('Could not read CSV:', e)\n",
    "        return\n",
    "    tmp.evaluation_data = df.to_dict('records')\n",
    "    plot_evaluation_results(tmp, save_dir=save_dir, show_figs=show_figs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will print the graphs and plots for each csv made\n",
    "# comment and uncommont out which one you want to open\n",
    "\n",
    "load_results_and_plot('Our-test/evaluation_results.csv', save_dir='evaluation_results_results', show_figs=True)\n",
    "#load_results_and_plot('Our-test/my_evaluation.csv', save_dir='my_evaluation_results', show_figs=True)\n",
    "#load_results_and_plot('Our-test/robustness_test.csv', save_dir='robustness_test_results', show_figs=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
